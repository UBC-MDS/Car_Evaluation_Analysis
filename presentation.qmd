---
title: "Predicting Level of Acceptability of Cars"
author: "Danish Karlin Isa, Nicholas Varabioff, Ximin Xu & Zuer Zhong"
date: today
format: revealjs
jupyter: python3
---

# Introduction

## Car Evaluation Data Target

Created by the efforts of M. Bohanec in the late 1980s

### Acceptability

1. `unacc`: Unacceptable. Cars that fail to meet basic criteria.
2. `acc`: Acceptable. Cars that meet minimum requirements.
3. `good`: Good. Cars that exceed average standards in some aspects.
4. `vgood`: Very good. Cars that meet the highest standards.

## Car Evaluation Data Features

Created by the efforts of M. Bohanec in the late 1980s

| Feature Name | Data Type | Values |
| ------------ | --------- | ------ |
| `buying` | Categorical | `low`, `med`, `high`, `vhigh` |
| `maint` | Categorical | `low`, `med`, `high`, `vhigh` |
| `doors` | Categorical | `2`, `3`, `4`, `5more` |
| `persons` | Categorical | `2`, `4`, `more` |
| `lug_boot` | Categorical | `small`, `med`, `big` |
| `safety` | Categorical | `low`, `med`, `high` |

# Exploratory data analysis

## Target Class Distribution

## Target Class Distribution

::: {.columns}

:::: {.column width="50%"}
- The dataset is imbalanced, with a majority of cars labeled as `unacceptable`.
- **Implications**:
  - Models might overpredict the `unacceptable` class.
  - Metrics like accuracy could be misleading, requiring metrics like Precision, Recall, and F1-score.
::::

:::: {.column width="50%"}
![Target Class Distribution](results/figures/target_distribution_raw.png){width=100%}
::::
:::

## Feature-Target Relationships

- Understanding how features influence the target variable:
  - Safety (`high`) correlates with higher acceptability.
  - Lower buying price (`low`) is often associated with `acceptable` or `good` cars.
![Features by Class](results/figures/feature_counts_by_class.png)


# Selecting our model

```{python}
# imports
import pandas as pd
from IPython.display import Markdown, display
from tabulate import tabulate

model_selection_results = pd.read_csv("results/tables/model_selection_results.csv").round(3)
```

## Shortlisting models from `sklearn`

| Type | Model |
| ---- | ----- |
| Baseline | `DummyClassifier` |
| Classification |  `DecisionTreeClassifier`, `KNearestNeighbors` |
| Hyperplanes | `SVC` |
| Probabilistic | `MultinomialNB` |
| Regression | `LogisticRegression` |

## Model selection with cross-validation {.smaller}

```{python}
Markdown(model_selection_results.to_markdown(index=False))
```

## The chosen model: `SVC`

Finding hyperplane that best separates the different classes (`quality`)

* "Curved" hyperplanes great for overlapping features

* Shorter `predict` times (versus `KNearestNeighbors`)

* Less susceptible to overfitting (versus `DecisionTreeClassifier`)

# Model tuning

Ximin's part goes here


# Conclusion

## Observations
1. Significant class imbalance with `unacceptable` being the dominant class.
2. Features like `safety` and `buying price` show clear relationships with the target class.

## Future improvements
   - Balance the dataset.
   - Introduce additional features or preprocess categorical variables for better granularity.
