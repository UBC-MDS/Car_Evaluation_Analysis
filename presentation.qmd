---
title: "Predicting Level of Acceptability of Cars"
author: "Danish Karlin Isa, Nicholas Varabioff, Ximin Xu & Zuer Zhong"
date: today
format: revealjs
jupyter: python3
bibliography: references.bib
---

# Introduction

## Car Evaluation Data Target

Created by the efforts of M. Bohanec in the late 1980s

### Acceptability

1. `unacc`: Unacceptable. Cars that fail to meet basic criteria.
2. `acc`: Acceptable. Cars that meet minimum requirements.
3. `good`: Good. Cars that exceed average standards in some aspects.
4. `vgood`: Very good. Cars that meet the highest standards.

## Car Evaluation Data Features

Created by the efforts of M. Bohanec in the late 1980s

| Feature Name | Data Type | Values |
| ------------ | --------- | ------ |
| `buying` | Categorical | `low`, `med`, `high`, `vhigh` |
| `maint` | Categorical | `low`, `med`, `high`, `vhigh` |
| `doors` | Categorical | `2`, `3`, `4`, `5more` |
| `persons` | Categorical | `2`, `4`, `more` |
| `lug_boot` | Categorical | `small`, `med`, `big` |
| `safety` | Categorical | `low`, `med`, `high` |

# Exploratory Data Analysis

## Target Class Distribution

## Target Class Distribution

::: {.columns}

:::: {.column width="50%"}
- The dataset is imbalanced, with a majority of cars labeled as `unacceptable`.
- **Implications**:
  - Models might overpredict the `unacceptable` class.
  - Metrics like accuracy could be misleading, requiring metrics like Precision, Recall, and F1-score.
::::

:::: {.column width="50%"}
![Target Class Distribution](results/figures/target_distribution_raw.png){width=100%}
::::
:::

## Feature-Target Relationships

- Understanding how features influence the target variable:
  - Safety (`high`) correlates with higher acceptability.
  - Lower buying price (`low`) is often associated with `acceptable` or `good` cars.
![Features by Class](results/figures/feature_counts_by_class.png)


# Selecting Our Model

```{python}
# imports
import pandas as pd
from IPython.display import Markdown, display
from tabulate import tabulate

model_selection_results = pd.read_csv("results/tables/model_selection_results.csv").round(3)
```

## Shortlisting models from `sklearn`

| Type | Model |
| ---- | ----- |
| Baseline | `DummyClassifier` |
| Classification |  `DecisionTreeClassifier`, `KNearestNeighbors` |
| Hyperplanes | `SVC` |
| Probabilistic | `MultinomialNB` |
| Regression | `LogisticRegression` |

## Model selection with cross-validation {.smaller}

```{python}
Markdown(model_selection_results.to_markdown(index=False))
```

## The chosen model: `SVC`

Finding hyperplane that best separates the different classes (`quality`)

* "Curved" hyperplanes great for overlapping features

* Shorter `predict` times (versus `KNearestNeighbors`)

* Less susceptible to overfitting (versus `DecisionTreeClassifier`)

# Model Tuning

## SVC RBF model has 2 hyperparameters:

::: {.columns}

::: {.column width="50%"}
- **`C`**: Controls the trade-off between complexity and accuracy.
- **`gamma`**: How far the influence of a single training example reaches.
:::

::: {.column width="50%"}
![Image of same data with different hyperparameters of SVC RBF](img/hyperpara.png){width=100%}[@scikit-learn]
:::

:::

## Our random search result: 
![Heatmap of test scores obtained during hyperparameter optimisation](../results/figures/car_hyperparameter.png){width=60%}
```{python}
import pickle
test_scores_df = pd.read_csv("./results/tables/test_scores.csv").round(2)
classification_report = pd.read_csv("./results/tables/classification_report.csv").round(3)
with open('./results/models/car_analysis.pickle', 'rb') as f:
    car_eva = pickle.load(f)
model_selection_results = pd.read_csv("./results/tables/model_selection_results.csv").round(3)

print()
```

`C` and `gamma` are `{python} car_eva.best_params_['svc__C']` and `{python} car_eva.best_params_['svc__gamma']`

## Fit the model and result

```{python}
#| tbl-cap: Classification report of the model

Markdown(classification_report.to_markdown(index=False))
```

# Conclusion

## Observations
1. Significant class imbalance with `unacceptable` being the dominant class.
2. Features like `safety` and `buying price` show clear relationships with the target class.

## Future Improvements
   - Balance the dataset.
   - Introduce additional features or preprocess categorical variables for better granularity.

## References
